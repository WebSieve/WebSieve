<div align="center">

# ğŸ§  Machine Learning & AI Engineering Portfolio

### _A Comprehensive Collection of Deep Learning, Transformers, and Neural Network Implementations_

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.0+-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)](https://tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg?style=for-the-badge)](LICENSE)

_From state-of-the-art transformer architectures to classical neural networks - a journey through modern AI_

[ğŸš€ Projects](#-featured-projects) â€¢ [ğŸ—ï¸ Architecture](#ï¸-architectures) â€¢ [ğŸ“Š Applications](#-applications) â€¢ [ğŸ› ï¸ Technologies](#ï¸-technologies)

</div>

---

## ğŸ“– Table of Contents

- [Overview](#-overview)
- [Featured Projects](#-featured-projects)
  - [DeepSeek V3 Recreation](#-deepseek-v3-recreation)
  - [Transformer Summarizer](#-transformer-summarizer)
  - [Decoder-Only Transformer](#-decoder-only-transformer)
  - [Neural Network Projects](#-neural-network-projects)
- [Architectures Implemented](#ï¸-architectures-implemented)
- [Applications & Use Cases](#-applications--use-cases)
- [Technologies & Frameworks](#ï¸-technologies--frameworks)
- [Project Structure](#-project-structure)
- [Getting Started](#-getting-started)
- [Highlights & Achievements](#-highlights--achievements)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸŒŸ Overview

<div align="center">

![AI Banner](https://images.unsplash.com/photo-1677442136019-21780ecad995?auto=format&fit=crop&w=1200&q=80)

</div>

This repository represents a comprehensive exploration of modern machine learning and artificial intelligence, featuring **cutting-edge transformer architectures**, **classical neural networks**, and **production-ready implementations** of state-of-the-art models.

### ğŸ¯ What You'll Find Here

- ğŸ§  **Advanced Transformer Models**: From-scratch implementations of GPT-style decoders, BERT-style encoders, and innovative architectures
- ğŸ”¬ **Research Reproductions**: Faithful recreations of papers like DeepSeek V3 with Multi-Head Latent Attention and Mixture of Experts
- ğŸ“Š **Diverse Applications**: Computer vision, NLP, time series, classification, detection, and more
- ğŸ“ **Educational Code**: Extensively documented implementations perfect for learning and teaching
- âš¡ **Production-Ready**: Complete training pipelines, checkpointing, evaluation metrics, and deployment-ready code

---

## ğŸš€ Featured Projects

### ğŸ§¬ DeepSeek V3 Recreation

<div align="center">

[![DeepSeek](https://img.shields.io/badge/Status-Active-success?style=flat-square)](DeepSeek_V3_Recreation/)
[![Paper](https://img.shields.io/badge/Paper-Implementation-blue?style=flat-square)](https://arxiv.org/abs/2401.00000)

</div>

> Note : Some components are still missing and will be added in the future.

A clean, educational PyTorch implementation of **DeepSeek V3's core architectural innovations**, pushing the boundaries of efficient transformer design.

**ğŸ”‘ Key Innovations:**
- **Multi-Head Latent Attention (MLA)**: Low-rank key-value compression reducing KV cache by 4-8x
- **Mixture of Experts (MoE)**: Sparse computation with top-k routing and load balancing
- **RMS Normalization**: Faster and more stable than traditional LayerNorm
- **Modular Design**: Clean separation of concerns for easy experimentation

**ğŸ“ Components:**
```
DeepSeek_V3_Recreation/
â”œâ”€â”€ multi_head_latent_attention.py  # Efficient attention with compression
â”œâ”€â”€ Mixture_Of_Experts.py           # Sparse expert routing
â”œâ”€â”€ DS_Block.py                      # Complete DeepSeek transformer block
â””â”€â”€ RMS_Norm.py                      # Root Mean Square normalization
```

**ğŸ’¡ Technical Highlights:**
- LoRA-style K/V decomposition for memory efficiency
- Separate RoPE and non-RoPE component handling
- SwiGLU activation in expert networks
- Auxiliary loss-free load balancing

---

### âœï¸ Transformer Summarizer

<div align="center">

[![Summarizer](https://img.shields.io/badge/Status-In%20Progress-yellow?style=flat-square)](Transformer_Summarizer/)
[![Encoder](https://img.shields.io/badge/Type-Encoder--Based-informational?style=flat-square)](Transformer_Summarizer/)

</div>

> Note : This project is unstructured. It will be completed soon.

An **encoder-based transformer** designed for text summarization and understanding tasks, featuring state-of-the-art attention mechanisms.

**ğŸ¯ Purpose:**
- Extractive and abstractive summarization
- Document understanding and encoding
- Semantic text representation

**ğŸ“¦ Current Components:**
```
Transformer_Summarizer/
â”œâ”€â”€ Encoder.py                       # Core transformer encoder
â”œâ”€â”€ Multi_Head_Latent_Attention.py  # Advanced attention mechanism
â””â”€â”€ RMS_Norm.py                      # Normalization layer
```

**ğŸ”§ Features:**
- Multi-head latent attention for efficiency
- Flexible encoder stack architecture
- Support for various text encoding tasks

---

### ğŸ¤– Decoder-Only Transformer

<div align="center">

[![Transformer](https://img.shields.io/badge/Status-Complete-success?style=flat-square)](DecoderOnlyTransformer_Basic/)
[![GPT](https://img.shields.io/badge/Style-GPT--like-blueviolet?style=flat-square)](DecoderOnlyTransformer_Basic/)

</div>

A **production-ready GPT-style transformer** built from scratch with modern architectural improvements and complete training infrastructure.

**â­ Standout Features:**

| Feature | Description |
|---------|-------------|
| ğŸ¯ **Rotary Position Embeddings (RoPE)** | Superior position encoding with better extrapolation |
| ğŸ”¥ **SwiGLU Activation** | Modern activation function used in LLaMA and PaLM |
| ğŸ“ **RMS Normalization** | Faster training convergence than LayerNorm |
| ğŸ² **Advanced Sampling** | Temperature, top-k, and nucleus sampling |
| ğŸ’¾ **Complete Training Pipeline** | Checkpointing, logging, learning rate scheduling |

**ğŸ“Š Training Capabilities:**
- Custom sliding window tokenization
- Efficient DataLoader implementation
- AdamW optimization with cosine annealing
- Gradient clipping and mixed precision support
- Validation loss tracking and early stopping

**ğŸ¨ Generation Methods:**
```python
# Temperature sampling
output = model.generate(prompt, temperature=0.8)

# Top-k sampling
output = model.generate(prompt, top_k=50)

# Flexible sequence lengths
output = model.generate(prompt, max_length=512)
```

---

### ğŸ§® Neural Network Projects

<div align="center">

[![NN](https://img.shields.io/badge/Projects-50+-red?style=flat-square)](Neural_Net_Projects/)
[![Diverse](https://img.shields.io/badge/Domains-CV%20%7C%20NLP%20%7C%20TS-orange?style=flat-square)](Neural_Net_Projects/)

</div>

A vast collection of **neural network implementations** spanning computer vision, natural language processing, time series, and more.

#### ğŸ–¼ï¸ Computer Vision

**Image Classification:**
- **MNIST Digit Recognition**: CNN-based digit classifier with 99%+ accuracy
- **CIFAR-10 Classification**: Multi-class image classification with data augmentation
- **CIFAR-100 Classification**: Fine-grained image classification with 100 classes
- **Transfer Learning**: Pre-trained model fine-tuning for custom datasets

**Object Detection:**
- Custom object detection implementations
- YOLO-style architectures
- Region-based CNN approaches

#### ğŸ“ Natural Language Processing

**Text Classification:**
- **Fake News Detection**: Multi-model ML approach with ensemble methods
- **Sentiment Analysis**: LSTM and transformer-based classifiers
- **Text Categorization**: Multi-class document classification

**Advanced NLP:**
- Positional embedding implementations
- Custom tokenizer demonstrations (GPT-2)
- Sequence-to-sequence models

#### ğŸ”¢ Classical ML & Specialized Tasks

**Classification Tasks:**
- **Iris Classification**: Multi-layer perceptron for species classification
- **Wine Classification**: Neural network for wine quality prediction
- **Cover Type Classification**: Forest cover type prediction
- **Higgs Boson Detection**: Binary classification for particle physics

**Specialized Applications:**
- **DNA Sequence Prediction**: Biological sequence modeling
- **Sin Wave Modeling**: Time series regression with neural networks
- **Cosine Function Approximation**: Function learning demonstrations

#### ğŸ—ï¸ Architecture Implementations

**State-of-the-Art Models:**
- Variational Autoencoders (VAE)
- Gaussian Mixture Models (GMM)
- Backpropagation from scratch
- Custom layer implementations

---

## ğŸ›ï¸ Architectures Implemented

<div align="center">

| Architecture | Type | Key Features | Status |
|-------------|------|--------------|--------|
| **DeepSeek V3** | Decoder | MLA, MoE, RMSNorm | âœ… Active |
| **GPT-Style Decoder** | Decoder | RoPE, SwiGLU, Causal Attention | âœ… Complete |
| **Transformer Encoder** | Encoder | Multi-Head Attention, Feed-Forward | ğŸŸ¡ In Progress |
| **CNN Architectures** | Vision | Conv Layers, Pooling, Batch Norm | âœ… Complete |
| **RNN/LSTM** | Sequential | Time Series, Text Generation | âœ… Complete |
| **VAE** | Generative | Latent Space, Reconstruction | ğŸŸ¡ In Progress |
| **Mixture of Experts** | Hybrid | Sparse Routing, Load Balancing | âœ… Complete |

</div>

### ğŸ”¬ Advanced Components

- **Attention Mechanisms**: Multi-head, multi-head latent, cross-attention, self-attention
- **Position Encodings**: Sinusoidal, learned, rotary (RoPE)
- **Normalization**: LayerNorm, BatchNorm, RMSNorm, GroupNorm
- **Activations**: ReLU, GELU, SwiGLU, Mish, Swish
- **Regularization**: Dropout, weight decay, gradient clipping, label smoothing

---

## ğŸ“Š Applications & Use Cases

### ğŸ¯ Computer Vision
```
âœ“ Image Classification (MNIST, CIFAR-10, CIFAR-100)
âœ“ Object Detection (Custom implementations)
âœ“ Transfer Learning (Pre-trained models)
âœ“ Feature Extraction
```

### ğŸ’¬ Natural Language Processing
```
âœ“ Text Generation (GPT-style models)
âœ“ Text Summarization (Encoder-based)
âœ“ Text Classification (Sentiment, Fake News)
âœ“ Language Modeling
```

### ğŸ“ˆ Time Series & Regression
```
âœ“ Function Approximation (Sin, Cosine)
âœ“ Sequence Prediction
âœ“ Regression Tasks
```

### ğŸ”¬ Research & Experimentation
```
âœ“ Architecture Ablation Studies
âœ“ Hyperparameter Optimization
âœ“ Novel Component Testing
âœ“ Paper Implementations
```

---

## ğŸ› ï¸ Technologies & Frameworks

<div align="center">

### Core Frameworks

![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![Keras](https://img.shields.io/badge/Keras-D00000?style=for-the-badge&logo=keras&logoColor=white)

### Scientific Computing

![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)
![SciPy](https://img.shields.io/badge/SciPy-8CAAE6?style=for-the-badge&logo=scipy&logoColor=white)

### Visualization & Analysis

![Matplotlib](https://img.shields.io/badge/Matplotlib-11557c?style=for-the-badge&logo=python&logoColor=white)
![Seaborn](https://img.shields.io/badge/Seaborn-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Plotly](https://img.shields.io/badge/Plotly-3F4F75?style=for-the-badge&logo=plotly&logoColor=white)

### Machine Learning

![scikit-learn](https://img.shields.io/badge/scikit--learn-F7931E?style=for-the-badge&logo=scikitlearn&logoColor=white)
![Jupyter](https://img.shields.io/badge/Jupyter-F37626?style=for-the-badge&logo=jupyter&logoColor=white)

</div>

---

## ğŸ“‚ Project Structure

```
ğŸ“ Machine Learning Portfolio
â”‚
â”œâ”€â”€ ğŸ§¬ DeepSeek_V3_Recreation/
â”‚   â”œâ”€â”€ Multi-Head Latent Attention
â”‚   â”œâ”€â”€ Mixture of Experts
â”‚   â”œâ”€â”€ Complete Transformer Blocks
â”‚   â””â”€â”€ RMS Normalization
â”‚
â”œâ”€â”€ âœï¸ Transformer_Summarizer/          [ğŸŸ¡ In Progress]
â”‚   â”œâ”€â”€ Encoder Architecture
â”‚   â”œâ”€â”€ Advanced Attention Mechanisms
â”‚   â””â”€â”€ Summarization Models
â”‚
â”œâ”€â”€ ğŸ¤– DecoderOnlyTransformer_Basic/
â”‚   â”œâ”€â”€ GPT-style Architecture
â”‚   â”œâ”€â”€ Training Pipeline
â”‚   â”œâ”€â”€ Text Generation
â”‚   â””â”€â”€ Sampling Strategies
â”‚
â”œâ”€â”€ ğŸ§  Neural_Net_Projects/
â”‚   â”œâ”€â”€ Computer Vision
â”‚   â”‚   â”œâ”€â”€ Image Classification
â”‚   â”‚   â”œâ”€â”€ Object Detection
â”‚   â”‚   â””â”€â”€ Transfer Learning
â”‚   â”‚
â”‚   â””â”€â”€ NLP & Text
â”‚       â”œâ”€â”€ Text Classification
â”‚       â”œâ”€â”€ Sequence Models
â”‚       â””â”€â”€ Custom Tokenizers
â”‚
â”œâ”€â”€ ğŸ“ Neural_Nets_and_Transformers/
â”‚   â”œâ”€â”€ Complete Implementations
â”‚   â”œâ”€â”€ Training Scripts
â”‚   â”œâ”€â”€ Evaluation Tools
â”‚   â””â”€â”€ Tutorial Notebooks
â”‚
â”œâ”€â”€ ğŸ”¬ Core Components/
â”‚   â”œâ”€â”€ Encoder.py / Decoder.py
â”‚   â”œâ”€â”€ Positional Embeddings
â”‚   â”œâ”€â”€ Attention Mechanisms
â”‚   â””â”€â”€ Custom Layers
â”‚
â””â”€â”€ ğŸ““ Jupyter Notebooks/
    â”œâ”€â”€ Anatomy Explorations
    â”œâ”€â”€ Architecture Visualizations
    â”œâ”€â”€ Training Demonstrations
    â””â”€â”€ Interactive Tutorials
```

---

## ğŸš€ Getting Started

### Prerequisites

```bash
Python 3.8+
PyTorch 2.0+
TensorFlow 2.0+
CUDA (optional, for GPU acceleration)
```

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/ml-portfolio.git
cd ml-portfolio

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Linux/Mac
# or
.venv\Scripts\activate  # On Windows

# Install dependencies
pip install torch torchvision torchaudio
pip install tensorflow
pip install numpy pandas scikit-learn matplotlib seaborn
pip install jupyter notebook ipykernel
```

### Quick Start Examples

#### 1ï¸âƒ£ Train a GPT-Style Model

```python
from DecoderOnlyTransformer_Basic.Transformer import Transformer
from DecoderOnlyTransformer_Basic.model_training import train_model

# Initialize model
model = Transformer(
    vocab_size=50000,
    embed_dim=512,
    num_layers=6,
    num_heads=8,
    ff_dim=2048
)

# Train
train_model(model, train_data, epochs=10)
```

#### 2ï¸âƒ£ Use DeepSeek V3 Components

```python
from DeepSeek_V3_Recreation.DS_Block import DS_Block

# Create a DeepSeek transformer block
block = DS_Block(
    hidden_size=512,
    num_heads=8,
    kv_lora_rank=64,
    num_experts=8,
    num_experts_per_token=2
)

# Forward pass
output = block(input_tensor)
```

#### 3ï¸âƒ£ Image Classification with CNN

```python
from Neural_Nets_and_Transformers.MNIST_digit_classification_with_CNN import build_model

# Load and train MNIST classifier
model = build_model()
model.fit(X_train, y_train, epochs=10, validation_split=0.2)
```

---

## ğŸ† Highlights & Achievements

<div align="center">

### ğŸ“Š By The Numbers

| Metric | Value |
|--------|-------|
| **Total Projects** | 50+ |
| **Architectures Implemented** | 15+ |
| **Lines of Code** | 25,000+ |
| **Jupyter Notebooks** | 30+ |
| **Research Papers Implemented** | 5+ |
| **Frameworks Mastered** | PyTorch, TensorFlow, Keras |

</div>

### ğŸŒŸ Key Achievements

- âœ… **Complete DeepSeek V3 Recreation**: Faithful implementation of cutting-edge architecture
- âœ… **Production-Ready Transformer**: Full training pipeline with state-of-the-art components
- âœ… **Diverse Application Portfolio**: CV, NLP, time series, and specialized tasks
- âœ… **Educational Quality**: Extensively documented code with clear explanations
- âœ… **Modern Best Practices**: RoPE, SwiGLU, RMSNorm, and more
- âœ… **Modular Design**: Reusable components for rapid experimentation

### ğŸ¯ Notable Implementations

1. **Multi-Head Latent Attention**: Memory-efficient attention with 4-8x KV cache reduction
2. **Mixture of Experts**: Sparse computation with intelligent routing
3. **Rotary Position Embeddings**: Superior position encoding for transformers
4. **Complete Training Infrastructure**: Checkpointing, logging, and evaluation pipelines
5. **Diverse Neural Architectures**: CNNs, RNNs, VAEs, and more

---

## ğŸ¤ Contributing

Contributions are welcome! Whether you want to:

- ğŸ› Report bugs
- ğŸ’¡ Suggest new features
- ğŸ“ Improve documentation
- ğŸ”§ Add new models or architectures
- âœ¨ Enhance existing implementations

Please feel free to open an issue or submit a pull request.

### Development Guidelines

1. Follow PEP 8 style guidelines
2. Add comprehensive docstrings
3. Include unit tests for new features
4. Update documentation as needed
5. Maintain modular, reusable code

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Research Papers**: DeepSeek V3, GPT, BERT, and countless others
- **Open Source Community**: PyTorch, TensorFlow, and Hugging Face teams
- **Educational Resources**: Stanford CS231n, CS224n, and fast.ai
- **Inspiration**: The broader ML/AI research community

---

## ğŸ“¬ Contact & Links

<div align="center">

[![GitHub](https://img.shields.io/badge/GitHub-Profile-181717?style=for-the-badge&logo=github)](https://github.com/WebSieve)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-0A66C2?style=for-the-badge&logo=linkedin)](https://linkedin.com/in/yourprofile)
[![Email](https://img.shields.io/badge/Email-Contact-EA4335?style=for-the-badge&logo=gmail&logoColor=white)](mailto:msahil2603@gmail.com)

### â­ Star this repo if you find it helpful!

</div>

---

<div align="center">

### ğŸš€ Built with passion for AI and Machine Learning

**"The only way to do great work is to love what you do."**

![Footer](https://capsule-render.vercel.app/api?type=waving&color=gradient&height=100&section=footer)

</div>
